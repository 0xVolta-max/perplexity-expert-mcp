#!/usr/bin/env node

import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import { z } from "zod";
import * as dotenv from "dotenv";
import { EXPERT_MODELS, TASK_TYPES, TaskType } from './config/models.js';
import { PerplexityService, PerplexityMessage } from './services/perplexity.js';
import { SmartRouter } from './utils/routing.js';

dotenv.config();

const perplexityService = new PerplexityService(process.env.PERPLEXITY_API_KEY || '');

const server = new McpServer({
  name: "Perplexity Pro Expert Delegation Server",
  version: "1.0.0"
});

const availableModels = Object.keys(EXPERT_MODELS) as [string, ...string[]];

// Tool 1: Expert Consultation
server.tool(
  "consult-expert",
  "Konsultiere ein spezifisches Expert-LLM √ºber Perplexity Pro f√ºr komplexe Aufgaben",
  {
    query: z.string().describe("Die komplexe Anfrage f√ºr den Experten"),
    expert_model: z.enum(availableModels).describe("Welches Expert-Modell verwendet werden soll"),
    context: z.string().optional().describe("Zus√§tzlicher Kontext f√ºr den Experten"),
    temperature: z.number().min(0).max(2).optional().describe("Temperature-Wert (0.0-2.0, default: Modell-spezifisch)"),
    task_type: z.enum(["analysis", "coding", "creative", "research", "problem-solving"]).optional().describe("Art der Aufgabe zur Prompt-Optimierung")
  },
  async ({ query, expert_model, context, temperature, task_type = "analysis" }) => {
    try {
      const modelConfig = EXPERT_MODELS[expert_model];
      if (!modelConfig) {
        throw new Error(`Unbekanntes Modell: ${expert_model}`);
      }

      const systemPrompts: Record<TaskType, string> = {
        analysis: "Du bist ein Experte f√ºr tiefgreifende Analysen. Liefere umfassende, datengest√ºtzte Erkenntnisse und strukturierte Bewertungen mit klaren Schlussfolgerungen.",
        coding: "Du bist ein Senior Software-Architekt und Coding-Experte. Liefere saubere, effiziente und gut dokumentierte L√∂sungen mit Best Practices. Erkl√§re deine Designentscheidungen.",
        creative: "Du bist ein kreativer Experte f√ºr innovative und originelle Inhalte. Denke au√üerhalb der gewohnten Bahnen und entwickle einzigartige, ansprechende L√∂sungen.",
        research: "Du bist ein Forschungsexperte, der pr√§zise, gut recherchierte Informationen und tiefgreifende Erkenntnisse liefert. Beziehe aktuelle Entwicklungen ein.",
        "problem-solving": "Du bist ein strategischer Probleml√∂ser, der komplexe Herausforderungen systematisch in umsetzbare L√∂sungen aufteilt. Ber√ºcksichtige verschiedene Perspektiven."
      };

      let expertPrompt = query;
      if (context) {
        expertPrompt = `**Kontext:**\n${context}\n\n**Aufgabe:**\n${query}`;
      }

      const messages: PerplexityMessage[] = [
        { role: "system", content: systemPrompts[task_type] },
        { role: "user", content: expertPrompt }
      ];

      const result = await perplexityService.callExpert(
        modelConfig,
        messages,
        temperature ?? modelConfig.defaultTemperature
      );
      if (!result) {
        throw new Error("Unerwartete API-Antwort: Kein Content erhalten");
      }

      const thinkingNote = modelConfig.reasoning
        ? "\n\nüß† **Extended Thinking aktiviert** - Dieses Modell hat tiefere Analyse-Schritte durchgef√ºhrt."
        : "";

      const strengthEmoji = {
        coding: "üíª",
        analysis: "üîç",
        multimodal: "üé®",
        research: "üìö",
        general: "üåü",
        creative: "‚ú®"
      }[modelConfig.strength] || "ü§ñ";

      return {
        content: [
          {
            type: "text",
            text: `# üéØ Expert Consultation Ergebnis\n\n## ${strengthEmoji} ${expert_model}\n\n${result}${thinkingNote}\n\n---\n\n**üìä Modell-Details:**\n- *${modelConfig.description}*\n- *üîß Task Type: ${task_type}*\n- *üå°Ô∏è Temperature: ${temperature ?? modelConfig.defaultTemperature}*\n- *‚ö° Powered by Perplexity Pro*`
          }
        ]
      };

    } catch (error: any) {
      return {
        content: [
          {
            type: "text",
            text: `# ‚ùå Expert Consultation Fehlgeschlagen\n\n**Modell:** ${expert_model}\n**Fehler:** ${error.message}\n\n## üí° M√∂gliche L√∂sungen:\n- √úberpr√ºfen Sie Ihren PERPLEXITY_API_KEY\n- Stellen Sie sicher, dass Ihr Pro-Abo aktiv ist\n- Das gew√§hlte Modell k√∂nnte tempor√§r nicht verf√ºgbar sein\n- Bei Timeout: Versuchen Sie eine k√ºrzere/einfachere Anfrage`
          }
        ],
        isError: true
      };
    }
  }
);

// Tool 2: Smart Expert Router
server.tool(
  "smart-expert-route",
  "Automatische Auswahl und Konsultation des besten Expert-Modells basierend auf Anfrage-Analyse",
  {
    query: z.string().describe("Die Anfrage zur Analyse und Weiterleitung an den optimalen Experten"),
    context: z.string().optional().describe("Zus√§tzlicher Kontext f√ºr die Analyse"),
    force_thinking: z.boolean().optional().describe("Erzwinge Verwendung von Reasoning-Modellen")
  },
  async ({ query, context, force_thinking = false }) => {
    try {
      const routing = SmartRouter.analyzeAndRoute(query, force_thinking);
      const modelConfig = EXPERT_MODELS[routing.selectedModel];
      if (!modelConfig) {
        throw new Error(`Modell-Konfiguration f√ºr ${routing.selectedModel} nicht gefunden`);
      }

      const systemPrompts: Record<TaskType, string> = {
        analysis: "Du bist ein Experte f√ºr umfassende Analysen und pr√§zise Bewertungen.",
        coding: "Du bist ein Senior Software-Experte f√ºr saubere, effiziente L√∂sungen.",
        "problem-solving": "Du bist ein strategischer Probleml√∂ser f√ºr komplexe Herausforderungen.",
        research: "Du bist ein Forschungsexperte f√ºr pr√§zise, aktuelle Informationen.",
        creative: "Du bist ein kreativer Experte f√ºr innovative Inhalte."
      };

      let expertPrompt = query;
      if (context) {
        expertPrompt = `**Kontext:**\n${context}\n\n**Aufgabe:**\n${query}`;
      }

      const messages: PerplexityMessage[] = [
        { role: "system", content: systemPrompts[routing.taskType] },
        { role: "user", content: expertPrompt }
      ];

      const result = await perplexityService.callExpert(modelConfig, messages);
      if (!result) {
        throw new Error("Unerwartete API-Antwort: Kein Content erhalten");
      }

      const thinkingNote = modelConfig.reasoning
        ? "\n\nüß† **Extended Thinking aktiviert**"
        : "";

      const confidenceBar = "‚ñà".repeat(Math.floor(routing.confidence * 10)) +
        "‚ñë".repeat(10 - Math.floor(routing.confidence * 10));

      const strengthEmoji = {
        coding: "üíª",
        analysis: "üîç",
        multimodal: "üé®",
        research: "üìö",
        general: "üåü",
        creative: "‚ú®"
      }[modelConfig.strength] || "ü§ñ";

      return {
        content: [
          {
            type: "text",
            text: `# ü§ñ Smart Expert Routing Ergebnis\n\n## üéØ Routing-Analyse\n- **Gew√§hltes Modell:** ${strengthEmoji} ${routing.selectedModel}\n- **Begr√ºndung:** ${routing.reasoning}\n- **Task-Typ:** ${routing.taskType}\n- **Confidence:** ${confidenceBar} ${(routing.confidence * 100).toFixed(0)}%\n\n## üé≠ Expert Response\n\n${result}${thinkingNote}\n\n---\n\n**üìä Modell-Details:**\n- *${modelConfig.description}*\n- *‚ö° Powered by Perplexity Pro*`
          }
        ]
      };

    } catch (error: any) {
      return {
        content: [
          {
            type: "text",
            text: `# ‚ùå Smart Routing Fehlgeschlagen\n\n**Fehler:** ${error.message}\n\nBitte √ºberpr√ºfen Sie Ihre Konfiguration und versuchen Sie es erneut.`
          }
        ],
        isError: true
      };
    }
  }
);

// Tool 3: Multi-Expert Comparison
server.tool(
  "compare-experts",
  "Vergleiche Antworten von mehreren Expert-Modellen parallel √ºber Perplexity Pro",
  {
    query: z.string().describe("Die Anfrage, die an mehrere Experten gesendet werden soll"),
    models: z.array(z.enum(availableModels))
      .min(2)
      .max(4)
      .describe("Welche Modelle verglichen werden sollen (2-4 Modelle)")
      .default(["claude-4-sonnet", "gemini-2.5-pro"]),
    context: z.string().optional().describe("Zus√§tzlicher Kontext f√ºr alle Experten"),
    temperature: z.number().min(0).max(2).optional().describe("Temperature f√ºr alle Modelle")
  },
  async ({ query, models, context, temperature }) => {
    try {
      const uniqueModels = [...new Set(models)];
      const results = await Promise.allSettled(
        uniqueModels.map(async (modelName) => {
          const modelConfig = EXPERT_MODELS[modelName];
          if (!modelConfig) {
            throw new Error(`Modell-Konfiguration f√ºr ${modelName} nicht gefunden`);
          }

          let expertPrompt = query;
          if (context) {
            expertPrompt = `**Kontext:**\n${context}\n\n**Aufgabe:**\n${query}`;
          }

          const messages: PerplexityMessage[] = [
            {
              role: "system",
              content: "Du bist ein AI-Experte, der detaillierte, pr√§zise Analysen liefert. Fokussiere dich auf deine St√§rken und liefere eine einzigartige Perspektive."
            },
            {
              role: "user",
              content: expertPrompt
            }
          ];

          const result = await perplexityService.callExpert(
            modelConfig,
            messages,
            temperature ?? modelConfig.defaultTemperature
          );
          if (!result) {
            throw new Error("Unerwartete API-Antwort: Kein Content erhalten");
          }
          return {
            model: modelName,
            result,
            config: modelConfig
          };
        })
      );

      let comparisonText = "# üèÜ Expert Model Vergleich\n\n";
      results.forEach((result, index) => {
        if (result.status === "fulfilled") {
          const { model, result: response, config } = result.value;
          const thinkingIcon = config.reasoning ? "üß†" : "‚ö°";
          const strengthIcon = {
            coding: "üíª",
            analysis: "üîç",
            multimodal: "üé®",
            research: "üìö",
            general: "üåü",
            creative: "‚ú®"
          }[config.strength] || "ü§ñ";
          comparisonText += `## ${thinkingIcon} ${strengthIcon} ${model}\n`;
          comparisonText += `*${config.description}*\n\n`;
          comparisonText += `${response}\n\n`;
          if (config.reasoning) {
            comparisonText += "*üß† Extended Thinking wurde verwendet*\n\n";
          }
          comparisonText += `---\n\n`;
        } else {
          const modelName = uniqueModels[index];
          comparisonText += `## ‚ùå ${modelName} (Fehlgeschlagen)\n`;
          comparisonText += `*Fehler: ${result.reason instanceof Error ? result.reason.message : result.reason}*\n\n---\n\n`;
        }
      });

      const successCount = results.filter(r => r.status === "fulfilled").length;
      comparisonText += `*üîß ${successCount}/${uniqueModels.length} Modelle erfolgreich | ‚ö° Powered by Perplexity Pro*`;

      return {
        content: [
          {
            type: "text",
            text: comparisonText
          }
        ]
      };

    } catch (error: any) {
      return {
        content: [
          {
            type: "text",
            text: `# ‚ùå Model Vergleich Fehlgeschlagen\n\n**Fehler:** ${error.message}`
          }
        ],
        isError: true
      };
    }
  }
);

// Tool 4: Model Info
server.tool(
  "model-info",
  "Zeige Informationen √ºber verf√ºgbare Expert-Modelle",
  {},
  async () => {
    try {
      let infoText = "# üìä Verf√ºgbare Expert-Modelle\n\n";
      for (const [modelName, config] of Object.entries(EXPERT_MODELS)) {
        const strengthIcon = {
          coding: "üíª",
          analysis: "üîç",
          multimodal: "üé®",
          research: "üìö",
          general: "üåü",
          creative: "‚ú®"
        }[config.strength] || "ü§ñ";
        const reasoningIcon = config.reasoning ? "üß†" : "‚ö°";
        infoText += `## ${reasoningIcon} ${strengthIcon} ${modelName}\n`;
        infoText += `- **Beschreibung:** ${config.description}\n`;
        infoText += `- **St√§rke:** ${config.strength}\n`;
        infoText += `- **Reasoning:** ${config.reasoning ? "Ja (Extended Thinking)" : "Nein"}\n`;
        infoText += `- **Max Tokens:** ${config.maxTokens}\n`;
        infoText += `- **Default Temperature:** ${config.defaultTemperature}\n\n`;
      }
      infoText += "\n## üéØ Task Types\n\n";
      for (const [taskType, description] of Object.entries(TASK_TYPES)) {
        infoText += `- **${taskType}:** ${description}\n`;
      }
      infoText += "\n*‚ö° Alle Modelle werden √ºber Perplexity Pro API bereitgestellt*";
      return {
        content: [
          {
            type: "text",
            text: infoText
          }
        ]
      };
    } catch (error: any) {
      return {
        content: [
          {
            type: "text",
            text: `‚ùå Fehler beim Abrufen der Modell-Informationen: ${error.message}`
          }
        ],
        isError: true
      };
    }
  }
);

async function main() {
  try {
    const connectionOk = await perplexityService.testConnection();
    if (!connectionOk) {
      console.error("‚ö†Ô∏è  Warnung: Verbindung zu Perplexity API konnte nicht getestet werden");
    }
    const transport = new StdioServerTransport();
    await server.connect(transport);
    console.error("üöÄ Perplexity Pro Expert Delegation MCP Server l√§uft");
    console.error("üéØ Verf√ºgbare Modelle:", Object.keys(EXPERT_MODELS).join(", "));
    console.error("‚ö° Powered by Perplexity Pro API");
  } catch (error) {
    console.error("‚ùå Fehler beim Starten des Servers:", error);
    process.exit(1);
  }
}

main().catch(console.error);
